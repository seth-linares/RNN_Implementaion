{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XtiXE04uGB_U"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8RbnIjwHGoR",
        "outputId": "01e42227-5d7b-40f6-e33a-0b9007e51955"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/sethlinares/rnn-texts-cse450/main/austen_plus_shakespeare.txt\n",
            "10397229/10397229 [==============================] - 0s 0us/step\n",
            "Length of text: 10296344 characters\n"
          ]
        }
      ],
      "source": [
        "path_to_file = tf.keras.utils.get_file('austen_plus_shakespeare.txt', 'https://raw.githubusercontent.com/sethlinares/rnn-texts-cse450/main/austen_plus_shakespeare.txt')\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print('Length of text: {} characters'.format(len(text)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XRnt0XUHUrq",
        "outputId": "87aeef91-4603-4592-bde5-72dd921cac57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VOLUME I\n",
            "\n",
            "\n",
            "\n",
            "CHAPTER I\n",
            "\n",
            "\n",
            "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
            "and happy disposition, seemed to unite some of the best blessings of\n",
            "existence; and had lived nearly twenty-\n"
          ]
        }
      ],
      "source": [
        "print(text[:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SLd7l0HP1Po",
        "outputId": "0dde5ffd-2c46-4b2b-b7a6-4ac46dff9bd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "103 unique characters\n",
            "['\\t', '\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '}', '£', 'À', 'Æ', 'Ç', 'É', 'à', 'â', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'î', 'œ', '—', '‘', '’', '“', '”', '…']\n"
          ]
        }
      ],
      "source": [
        "vocab = sorted(set(text))\n",
        "print('{} unique characters'.format(len(vocab)))\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtjOxL7wQibb"
      },
      "outputs": [],
      "source": [
        "ids_from_chars = preprocessing.StringLookup(vocabulary=list(vocab))\n",
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True)\n",
        "\n",
        "def text_from_ids(ids):\n",
        "  joinedTensor = tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
        "  return joinedTensor.numpy().decode(\"utf-8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "52bkemreRw8q",
        "outputId": "6cfcb7aa-c586-4118-9fa3-d36f7351dcd0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'ids_from_chars' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-891446c8b504>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Now we'll verify that they work, by getting the code for \"A\", and then looking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# that up in reverse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtestids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mids_from_chars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"T\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"u\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"h\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtestids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ids_from_chars' is not defined"
          ]
        }
      ],
      "source": [
        "testids = ids_from_chars([\"T\", \"r\", \"u\", \"t\", \"h\"])\n",
        "testids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGUnSHjtD_IJ",
        "outputId": "9207e143-72ac-4568-bbcb-83c643d13ff0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5,), dtype=string, numpy=array([b'T', b'r', b'u', b't', b'h'], dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "chars_from_ids(testids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8rghkpLLLjL5",
        "outputId": "c12d103f-2fd6-4a15-8588-ab0165498ca9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Truth'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "testString = text_from_ids( testids )\n",
        "testString"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PLJWOg2P_fE",
        "outputId": "e8f4000c-6579-4251-94dd-b63687ef38d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10296344,), dtype=int64, numpy=array([48, 41, 38, ..., 45,  2,  2])>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-nBqVY6pFpZs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "cb0c1dc4-86c2-4dc8-b407-97d5a92bb130"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'all_ids' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-08aa36a46d52>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Now, convert that into a tensorflow dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mids_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'all_ids' is not defined"
          ]
        }
      ],
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Fr28CJxUBtG"
      },
      "outputs": [],
      "source": [
        "seq_length = 100\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poNVukmsUFkq",
        "outputId": "6e64e9f4-3544-455c-8c9e-8e78e7afe1e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:  VOLUME I\n",
            "\n",
            "\n",
            "\n",
            "CHAPTER I\n",
            "\n",
            "\n",
            "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
            "and happ\n",
            "--------\n",
            "Target:  OLUME I\n",
            "\n",
            "\n",
            "\n",
            "CHAPTER I\n",
            "\n",
            "\n",
            "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
            "and happy\n"
          ]
        }
      ],
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "    print(\"Input: \", text_from_ids(input_example))\n",
        "    print(\"--------\")\n",
        "    print(\"Target: \", text_from_ids(target_example))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "QDdr6xfZYa0o",
        "outputId": "1356c1b8-23fe-4efa-9266-f14cc28ff4b2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-04cb8e84647a>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m dataset = (\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBUFFER_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_remainder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 1024\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons"
      ],
      "metadata": {
        "id": "IQw_NEG5j_I0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d57ba85-d715-4c60-f9b0-c011bd8f1106"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/591.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m591.0/591.0 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.1)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.20.0 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fj4uh9y-Y9mx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35e27b75-839c-4d71-d207-13f6c2471e1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import tensorflow_addons as tfa\n",
        "class austen_plus_shakespeare(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__()\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.dropout1 = tf.keras.layers.Dropout(.1)\n",
        "    self.rnn1 = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True)\n",
        "    self.rnn2 = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(.1)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    x = self.dropout1(x, training=training)\n",
        "    if states is None:\n",
        "      states = [self.rnn1.get_initial_state(x), self.rnn2.get_initial_state(x)]\n",
        "    x, h1, c1 = self.rnn1(x, initial_state=states[0], training=training)\n",
        "    x, h2, c2 = self.rnn2(x, initial_state=states[1], training=training)\n",
        "\n",
        "    states = [(h1, c1), (h2, c2)]\n",
        "    x = self.dropout2(x, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x\n",
        "\n",
        "model = austen_plus_shakespeare(vocab_size=len(ids_from_chars.get_vocabulary()), embedding_dim=512, rnn_units=2048)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C67kN7YAdfSf",
        "outputId": "ff6d52c3-ce3a-450a-f5ee-c894100e444b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1024, 100, 104) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJGL8gCWdsiu",
        "outputId": "a37bc946-518c-4977-bc4e-496b80af18c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"austen_plus_shakespeare\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  53248     \n",
            "                                                                 \n",
            " dropout (Dropout)           multiple                  0         \n",
            "                                                                 \n",
            " lstm (LSTM)                 multiple                  20979712  \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               multiple                  33562624  \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         multiple                  0         \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  213096    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 54,808,680\n",
            "Trainable params: 54,808,680\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vOxc7CkaGQB"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
        "model.fit(dataset, epochs=150, callbacks=[early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A95Qkq72O5Yi",
        "outputId": "effa872f-a116-4036-e557-ef12720e631b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "99/99 [==============================] - 42s 357ms/step - loss: 3.3787\n",
            "Epoch 2/150\n",
            "99/99 [==============================] - 37s 357ms/step - loss: 2.4263\n",
            "Epoch 3/150\n",
            "99/99 [==============================] - 37s 357ms/step - loss: 2.0160\n",
            "Epoch 4/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 1.7396\n",
            "Epoch 5/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 1.5451\n",
            "Epoch 6/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 1.4290\n",
            "Epoch 7/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 1.3523\n",
            "Epoch 8/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 1.3019\n",
            "Epoch 9/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 1.2623\n",
            "Epoch 10/150\n",
            "99/99 [==============================] - 37s 359ms/step - loss: 1.2305\n",
            "Epoch 11/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 1.2032\n",
            "Epoch 12/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 1.1785\n",
            "Epoch 13/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 1.1564\n",
            "Epoch 14/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 1.1351\n",
            "Epoch 15/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 1.1142\n",
            "Epoch 16/150\n",
            "99/99 [==============================] - 37s 359ms/step - loss: 1.0927\n",
            "Epoch 17/150\n",
            "99/99 [==============================] - 37s 359ms/step - loss: 1.0713\n",
            "Epoch 18/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 1.0480\n",
            "Epoch 19/150\n",
            "99/99 [==============================] - 37s 359ms/step - loss: 1.0250\n",
            "Epoch 20/150\n",
            "99/99 [==============================] - 37s 359ms/step - loss: 1.0001\n",
            "Epoch 21/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.9744\n",
            "Epoch 22/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.9471\n",
            "Epoch 23/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.9194\n",
            "Epoch 24/150\n",
            "99/99 [==============================] - 37s 359ms/step - loss: 0.8889\n",
            "Epoch 25/150\n",
            "99/99 [==============================] - 37s 359ms/step - loss: 0.8594\n",
            "Epoch 26/150\n",
            "99/99 [==============================] - 37s 359ms/step - loss: 0.8281\n",
            "Epoch 27/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.7972\n",
            "Epoch 28/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.7678\n",
            "Epoch 29/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.7376\n",
            "Epoch 30/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.7067\n",
            "Epoch 31/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.6781\n",
            "Epoch 32/150\n",
            "99/99 [==============================] - 37s 359ms/step - loss: 0.6495\n",
            "Epoch 33/150\n",
            "99/99 [==============================] - 37s 359ms/step - loss: 0.6225\n",
            "Epoch 34/150\n",
            "99/99 [==============================] - 37s 359ms/step - loss: 0.5960\n",
            "Epoch 35/150\n",
            "99/99 [==============================] - 37s 359ms/step - loss: 0.5731\n",
            "Epoch 36/150\n",
            "99/99 [==============================] - 37s 359ms/step - loss: 0.5451\n",
            "Epoch 37/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.5236\n",
            "Epoch 38/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.5007\n",
            "Epoch 39/150\n",
            "99/99 [==============================] - 37s 359ms/step - loss: 0.4795\n",
            "Epoch 40/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.4606\n",
            "Epoch 41/150\n",
            "99/99 [==============================] - 37s 359ms/step - loss: 0.4426\n",
            "Epoch 42/150\n",
            "99/99 [==============================] - 37s 359ms/step - loss: 0.4263\n",
            "Epoch 43/150\n",
            "99/99 [==============================] - 37s 359ms/step - loss: 0.4093\n",
            "Epoch 44/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.3956\n",
            "Epoch 45/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.3820\n",
            "Epoch 46/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.3684\n",
            "Epoch 47/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.3563\n",
            "Epoch 48/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.3455\n",
            "Epoch 49/150\n",
            "99/99 [==============================] - 37s 359ms/step - loss: 0.3361\n",
            "Epoch 50/150\n",
            "99/99 [==============================] - 37s 359ms/step - loss: 0.3278\n",
            "Epoch 51/150\n",
            "99/99 [==============================] - 37s 359ms/step - loss: 0.3199\n",
            "Epoch 52/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.3124\n",
            "Epoch 53/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.3050\n",
            "Epoch 54/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.2998\n",
            "Epoch 55/150\n",
            "99/99 [==============================] - 37s 359ms/step - loss: 0.2938\n",
            "Epoch 56/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.2882\n",
            "Epoch 57/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.2841\n",
            "Epoch 58/150\n",
            "99/99 [==============================] - 37s 359ms/step - loss: 0.2794\n",
            "Epoch 59/150\n",
            "99/99 [==============================] - 37s 359ms/step - loss: 0.2757\n",
            "Epoch 60/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.2720\n",
            "Epoch 61/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.2677\n",
            "Epoch 62/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.2664\n",
            "Epoch 63/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.2623\n",
            "Epoch 64/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.2607\n",
            "Epoch 65/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.2589\n",
            "Epoch 66/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.2566\n",
            "Epoch 67/150\n",
            "99/99 [==============================] - 37s 359ms/step - loss: 0.2548\n",
            "Epoch 68/150\n",
            "99/99 [==============================] - 37s 359ms/step - loss: 0.2528\n",
            "Epoch 69/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.2509\n",
            "Epoch 70/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.2507\n",
            "Epoch 71/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.2490\n",
            "Epoch 72/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.2477\n",
            "Epoch 73/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.2467\n",
            "Epoch 74/150\n",
            "99/99 [==============================] - 37s 359ms/step - loss: 0.2447\n",
            "Epoch 75/150\n",
            "99/99 [==============================] - 37s 359ms/step - loss: 0.2436\n",
            "Epoch 76/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.2432\n",
            "Epoch 77/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.2431\n",
            "Epoch 78/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.2423\n",
            "Epoch 79/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.2407\n",
            "Epoch 80/150\n",
            "99/99 [==============================] - 37s 357ms/step - loss: 0.2408\n",
            "Epoch 81/150\n",
            "99/99 [==============================] - 37s 359ms/step - loss: 0.2402\n",
            "Epoch 82/150\n",
            "99/99 [==============================] - 37s 359ms/step - loss: 0.2392\n",
            "Epoch 83/150\n",
            "99/99 [==============================] - 37s 359ms/step - loss: 0.2391\n",
            "Epoch 84/150\n",
            "99/99 [==============================] - 37s 359ms/step - loss: 0.2381\n",
            "Epoch 85/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.2371\n",
            "Epoch 86/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.2367\n",
            "Epoch 87/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.2361\n",
            "Epoch 88/150\n",
            "99/99 [==============================] - 37s 357ms/step - loss: 0.2361\n",
            "Epoch 89/150\n",
            "99/99 [==============================] - 37s 357ms/step - loss: 0.2363\n",
            "Epoch 90/150\n",
            "99/99 [==============================] - 37s 357ms/step - loss: 0.2367\n",
            "Epoch 91/150\n",
            "99/99 [==============================] - 37s 359ms/step - loss: 0.2359\n",
            "Epoch 92/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.2356\n",
            "Epoch 93/150\n",
            "99/99 [==============================] - 37s 359ms/step - loss: 0.2352\n",
            "Epoch 94/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.2348\n",
            "Epoch 95/150\n",
            "99/99 [==============================] - 37s 359ms/step - loss: 0.2347\n",
            "Epoch 96/150\n",
            "99/99 [==============================] - 37s 357ms/step - loss: 0.2354\n",
            "Epoch 97/150\n",
            "99/99 [==============================] - 37s 357ms/step - loss: 0.2351\n",
            "Epoch 98/150\n",
            "99/99 [==============================] - 37s 357ms/step - loss: 0.2349\n",
            "Epoch 99/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.2342\n",
            "Epoch 100/150\n",
            "99/99 [==============================] - 37s 359ms/step - loss: 0.2339\n",
            "Epoch 101/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.2337\n",
            "Epoch 102/150\n",
            "99/99 [==============================] - 37s 357ms/step - loss: 0.2340\n",
            "Epoch 103/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.2334\n",
            "Epoch 104/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.2345\n",
            "Epoch 105/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.2335\n",
            "Epoch 106/150\n",
            "99/99 [==============================] - 37s 357ms/step - loss: 0.2338\n",
            "Epoch 107/150\n",
            "99/99 [==============================] - 37s 357ms/step - loss: 0.2336\n",
            "Epoch 108/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.2330\n",
            "Epoch 109/150\n",
            "99/99 [==============================] - 37s 358ms/step - loss: 0.2330\n",
            "Epoch 110/150\n",
            "99/99 [==============================] - 37s 357ms/step - loss: 0.2337\n",
            "Epoch 111/150\n",
            "99/99 [==============================] - 37s 357ms/step - loss: 0.2330\n",
            "Epoch 112/150\n",
            "99/99 [==============================] - 37s 357ms/step - loss: 0.2334\n",
            "Epoch 113/150\n",
            "99/99 [==============================] - 37s 357ms/step - loss: 0.2332\n",
            "Epoch 114/150\n",
            "99/99 [==============================] - 37s 359ms/step - loss: 0.2336\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f61cfd8abf0>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eJl3ry4glT7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install language-tool-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5x3LuFG0-1c",
        "outputId": "e297d370-7f51-42c4-fac2-922d292a335d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting language-tool-python\n",
            "  Downloading language_tool_python-2.7.1-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from language-tool-python) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from language-tool-python) (4.65.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (3.4)\n",
            "Installing collected packages: language-tool-python\n",
            "Successfully installed language-tool-python-2.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import language_tool_python\n",
        "\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=.5):\n",
        "    super().__init__()\n",
        "    self.temperature=temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    skip_ids = self.ids_from_chars(['','[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices = skip_ids,\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask,validate_indices=False)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    predicted_logits, states =  self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    return self.chars_from_ids(predicted_ids), states\n"
      ],
      "metadata": {
        "id": "VUTUfxud04AZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import difflib\n",
        "\n",
        "\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
        "\n",
        "\n",
        "states = None\n",
        "next_char = tf.constant(['The world seemed like such a peaceful place until the magic tree was discovered in London.'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "\n",
        "generated_text = result[0].numpy().decode('utf-8')\n",
        "print(\"Generated text before correction:\")\n",
        "print(generated_text)\n",
        "\n",
        "\n",
        "def correct_text(text):\n",
        "    tool = language_tool_python.LanguageTool('en-US')\n",
        "    corrected_text = tool.correct(text)\n",
        "    return corrected_text\n",
        "\n",
        "corrected_text = correct_text(generated_text)\n",
        "print(\"\\nGenerated text after correction:\")\n",
        "print(corrected_text)\n",
        "\n",
        "def print_diff(text1, text2):\n",
        "    diff = difflib.ndiff(text1.splitlines(keepends=True), text2.splitlines(keepends=True))\n",
        "    print(''.join(diff))\n",
        "print(\"\\nDifferences between the generated text and the corrected text:\")\n",
        "print_diff(generated_text, corrected_text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7T_9C9A-09h-",
        "outputId": "a0d85efe-dbf9-4c91-a40a-97d9c8868c8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text before correction:\n",
            "The world seemed like such a peaceful place until the magic tree was discovered in London.\n",
            "\n",
            "The only door and a new intends were false, as the fathers do, and on many\n",
            "account.  Mrs. Weston was the very month with child.  She had been used\n",
            "by either and Mrs. Dashwood, and they all come downstairs. Fanny was left with her\n",
            "in the land, the difference was great. Had she expected that one on the wedding night\n",
            "she rescended thus unborn in the custom, she said, ‘The pity of it,\n",
            "    then their mother’s order to another, to steal the stains.\n",
            "\n",
            "\n",
            "                   45\n",
            "\n",
            "The chine of all the fair ladies is grown,\n",
            "Good gracious war, her lips are comprehended,\n",
            "    Not many doubts and honourable lands.\n",
            "It is such a common that is honourable.\n",
            "What was the one?\n",
            "\n",
            "CORIOLANUS.\n",
            "The god of soldiers,\n",
            "With the consent of such time’s time to bear neglect\n",
            "What we can do now with thee on him they both\n",
            "To stop and through every occasion\n",
            "Now to have it but bounds and scorns to heaven.\n",
            "\n",
            "CHIRON.\n",
            "Thy counsel, lad, smells of no cowardice.\n",
            "\n",
            "DEMETRIUS.\n",
            "_Sit, savour thou of man! Awake, awake!\n",
            "\n",
            "CLAUDIO.\n",
            "Faith, k\n",
            "\n",
            "Generated text after correction:\n",
            "The world seemed like such a peaceful place until the magic tree was discovered in London.\n",
            "\n",
            "The only door and a new intends were false, as the fathers do, and on many accounts.  Mrs. Weston was the very month with child.  She had been used\n",
            "by either and Mrs. Dash wood, and they all come downstairs. Fanny was left with her\n",
            "in the land, the difference was great. Had she expected that one on the wedding night\n",
            "she descended thus unborn in the custom, she said, ‘The pity of it,\n",
            "    then their mother’s order to another, to steal the stains.\n",
            "\n",
            "\n",
            " 45\n",
            "\n",
            "The chine of all the fair ladies is grown,\n",
            "Good gracious war, her lips are comprehended,\n",
            "    Not many doubts and honorable lands.\n",
            "It is such a common that is honorable.\n",
            "What was the one?\n",
            "\n",
            "CORIOLANUS.\n",
            "The god of soldiers,\n",
            "With the consent of such time’s time to bear neglect\n",
            "What we can do now with thee on him, they both\n",
            "To stop and through every occasion\n",
            "Now to have it but bounds and scorns to heaven.\n",
            "\n",
            "CHIRON.\n",
            "Thy counsel, lad, smells of no cowardice.\n",
            "\n",
            "DEMETRIUS.\n",
            "_Sit, Seymour thou of man! Awake, awake!\n",
            "\n",
            "CLAUDIO.\n",
            "Faith, k\n",
            "\n",
            "Differences between the generated text and the corrected text:\n",
            "  The world seemed like such a peaceful place until the magic tree was discovered in London.\n",
            "  \n",
            "+ The only door and a new intends were false, as the fathers do, and on many accounts.  Mrs. Weston was the very month with child.  She had been used\n",
            "- The only door and a new intends were false, as the fathers do, and on many\n",
            "- account.  Mrs. Weston was the very month with child.  She had been used\n",
            "- by either and Mrs. Dashwood, and they all come downstairs. Fanny was left with her\n",
            "+ by either and Mrs. Dash wood, and they all come downstairs. Fanny was left with her\n",
            "?                        +\n",
            "  in the land, the difference was great. Had she expected that one on the wedding night\n",
            "- she rescended thus unborn in the custom, she said, ‘The pity of it,\n",
            "?     ^\n",
            "+ she descended thus unborn in the custom, she said, ‘The pity of it,\n",
            "?     ^\n",
            "      then their mother’s order to another, to steal the stains.\n",
            "  \n",
            "  \n",
            "-                    45\n",
            "+  45\n",
            "  \n",
            "  The chine of all the fair ladies is grown,\n",
            "  Good gracious war, her lips are comprehended,\n",
            "-     Not many doubts and honourable lands.\n",
            "?                             -\n",
            "+     Not many doubts and honorable lands.\n",
            "- It is such a common that is honourable.\n",
            "?                                 -\n",
            "+ It is such a common that is honorable.\n",
            "  What was the one?\n",
            "  \n",
            "  CORIOLANUS.\n",
            "  The god of soldiers,\n",
            "  With the consent of such time’s time to bear neglect\n",
            "- What we can do now with thee on him they both\n",
            "+ What we can do now with thee on him, they both\n",
            "?                                    +\n",
            "  To stop and through every occasion\n",
            "  Now to have it but bounds and scorns to heaven.\n",
            "  \n",
            "  CHIRON.\n",
            "  Thy counsel, lad, smells of no cowardice.\n",
            "  \n",
            "  DEMETRIUS.\n",
            "- _Sit, savour thou of man! Awake, awake!\n",
            "?       ^^^\n",
            "+ _Sit, Seymour thou of man! Awake, awake!\n",
            "?       ^^^^\n",
            "  \n",
            "  CLAUDIO.\n",
            "  Faith, k\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}